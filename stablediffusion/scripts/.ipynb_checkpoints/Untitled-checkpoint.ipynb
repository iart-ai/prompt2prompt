{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a4d2c7-aafa-4c2a-b0e4-413580cc458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "from einops import repeat, rearrange\n",
    "from pytorch_lightning import seed_everything\n",
    "from imwatermark import WatermarkEncoder\n",
    "\n",
    "from scripts.txt2img import put_watermark\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.data.util import AddMiDaS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db258767-b301-4048-9483-6226db5a5768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f5a182ef250>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False) # 设置一个上下文环境，tensor运算产生的新的节点都是不可求导的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b0af18-0b59-4bce-8980-107d3e6a8369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/prompt2prompt/stablediffusion\n"
     ]
    }
   ],
   "source": [
    "%cd autodl-tmp/prompt2prompt/stablediffusion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b55d7e30-76a4-4d21-9f2a-337eda04f2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDepth2ImageDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "def initialize_model(config, ckpt):\n",
    "    config = OmegaConf.load(config)\n",
    "    model = instantiate_from_config(config.model)\n",
    "    model.load_state_dict(torch.load(ckpt)[\"state_dict\"], strict=False)\n",
    "\n",
    "    device = torch.device(\n",
    "        \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    sampler = DDIMSampler(model)\n",
    "    return sampler\n",
    "\n",
    "sampler = initialize_model(\"configs/stable-diffusion/v2-midas-inference.yaml\", \"512-depth-ema.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "912fb9e2-d592-4d7e-a69a-a97b0db60e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_sd(\n",
    "        image,\n",
    "        txt,\n",
    "        device,\n",
    "        num_samples=1,\n",
    "        model_type=\"dpt_hybrid\"\n",
    "):\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    image = torch.from_numpy(image).to(dtype=torch.float32) / 127.5 - 1.0\n",
    "    # sample['jpg'] is tensor hwc in [-1, 1] at this point\n",
    "    midas_trafo = AddMiDaS(model_type=model_type)\n",
    "    batch = {\n",
    "        \"jpg\": image,\n",
    "        \"txt\": num_samples * [txt],\n",
    "    }\n",
    "    batch = midas_trafo(batch)\n",
    "    batch[\"jpg\"] = rearrange(batch[\"jpg\"], 'h w c -> 1 c h w')\n",
    "    batch[\"jpg\"] = repeat(batch[\"jpg\"].to(device=device),\n",
    "                          \"1 ... -> n ...\", n=num_samples)\n",
    "    batch[\"midas_in\"] = repeat(torch.from_numpy(batch[\"midas_in\"][None, ...]).to(\n",
    "        device=device), \"1 ... -> n ...\", n=num_samples)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def paint(sampler, image, prompt, t_enc, seed, scale, num_samples=1, callback=None,\n",
    "          do_full_sample=False):\n",
    "    device = torch.device(\n",
    "        \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = sampler.model\n",
    "    seed_everything(seed)\n",
    "\n",
    "    print(\"Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\")\n",
    "    wm = \"SDV2\"\n",
    "    wm_encoder = WatermarkEncoder()\n",
    "    wm_encoder.set_watermark('bytes', wm.encode('utf-8'))\n",
    "\n",
    "    with torch.no_grad(),\\\n",
    "            torch.autocast(\"cuda\"):\n",
    "        batch = make_batch_sd(\n",
    "            image, txt=prompt, device=device, num_samples=num_samples)\n",
    "        z = model.get_first_stage_encoding(model.encode_first_stage(\n",
    "            batch[model.first_stage_key]))  # move to latent space\n",
    "        c = model.cond_stage_model.encode(batch[\"txt\"])\n",
    "        c_cat = list()\n",
    "        for ck in model.concat_keys:\n",
    "            cc = batch[ck]\n",
    "            cc = model.depth_model(cc)\n",
    "            depth_min, depth_max = torch.amin(cc, dim=[1, 2, 3], keepdim=True), torch.amax(cc, dim=[1, 2, 3],\n",
    "                                                                                           keepdim=True)\n",
    "            display_depth = (cc - depth_min) / (depth_max - depth_min)\n",
    "            depth_image = Image.fromarray(\n",
    "                (display_depth[0, 0, ...].cpu().numpy() * 255.).astype(np.uint8))\n",
    "            cc = torch.nn.functional.interpolate(\n",
    "                cc,\n",
    "                size=z.shape[2:],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            depth_min, depth_max = torch.amin(cc, dim=[1, 2, 3], keepdim=True), torch.amax(cc, dim=[1, 2, 3],\n",
    "                                                                                           keepdim=True)\n",
    "            cc = 2. * (cc - depth_min) / (depth_max - depth_min) - 1.\n",
    "            c_cat.append(cc)\n",
    "        c_cat = torch.cat(c_cat, dim=1)\n",
    "        # cond\n",
    "        cond = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n",
    "\n",
    "        # uncond cond\n",
    "        uc_cross = model.get_unconditional_conditioning(num_samples, \"\")\n",
    "        uc_full = {\"c_concat\": [c_cat], \"c_crossattn\": [uc_cross]}\n",
    "        if not do_full_sample:\n",
    "            # encode (scaled latent)\n",
    "            z_enc = sampler.stochastic_encode(\n",
    "                z, torch.tensor([t_enc] * num_samples).to(model.device))\n",
    "        else:\n",
    "            z_enc = torch.randn_like(z)\n",
    "        # decode it\n",
    "        samples = sampler.decode(z_enc, cond, t_enc, unconditional_guidance_scale=scale,\n",
    "                                 unconditional_conditioning=uc_full, callback=callback)\n",
    "        x_samples_ddim = model.decode_first_stage(samples)\n",
    "        result = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "        result = result.cpu().numpy().transpose(0, 2, 3, 1) * 255\n",
    "    return [depth_image] + [put_watermark(Image.fromarray(img.astype(np.uint8)), wm_encoder) for img in result]\n",
    "\n",
    "\n",
    "def pad_image(input_image):\n",
    "    pad_w, pad_h = np.max(((2, 2), np.ceil(\n",
    "        np.array(input_image.size) / 64).astype(int)), axis=0) * 64 - input_image.size\n",
    "    im_padded = Image.fromarray(\n",
    "        np.pad(np.array(input_image), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n",
    "    return im_padded\n",
    "\n",
    "\n",
    "def predict(input_image, prompt, steps, num_samples, scale, seed, eta, strength):\n",
    "    init_image = input_image.convert(\"RGB\")\n",
    "    image = pad_image(init_image)  # resize to integer multiple of 32\n",
    "\n",
    "    sampler.make_schedule(steps, ddim_eta=eta, verbose=True)\n",
    "    assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "    do_full_sample = strength == 1.\n",
    "    t_enc = min(int(strength * steps), steps-1)\n",
    "    result = paint(\n",
    "        sampler=sampler,\n",
    "        image=image,\n",
    "        prompt=prompt,\n",
    "        t_enc=t_enc,\n",
    "        seed=seed,\n",
    "        scale=scale,\n",
    "        num_samples=num_samples,\n",
    "        callback=None,\n",
    "        do_full_sample=do_full_sample\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39dfd9af-193b-44ca-9c0c-bf5b7f734f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 8888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected timesteps for ddim sampler: [  1  21  41  61  81 101 121 141 161 181 201 221 241 261 281 301 321 341\n",
      " 361 381 401 421 441 461 481 501 521 541 561 581 601 621 641 661 681 701\n",
      " 721 741 761 781 801 821 841 861 881 901 921 941 961 981]\n",
      "Selected alphas for ddim sampler: a_t: tensor([0.9983, 0.9804, 0.9609, 0.9398, 0.9171, 0.8930, 0.8674, 0.8404, 0.8121,\n",
      "        0.7827, 0.7521, 0.7207, 0.6885, 0.6557, 0.6224, 0.5888, 0.5551, 0.5215,\n",
      "        0.4882, 0.4552, 0.4229, 0.3913, 0.3605, 0.3308, 0.3023, 0.2750, 0.2490,\n",
      "        0.2245, 0.2014, 0.1799, 0.1598, 0.1413, 0.1243, 0.1087, 0.0946, 0.0819,\n",
      "        0.0705, 0.0604, 0.0514, 0.0435, 0.0365, 0.0305, 0.0254, 0.0210, 0.0172,\n",
      "        0.0140, 0.0113, 0.0091, 0.0073, 0.0058]); a_(t-1): [0.99914998 0.99829602 0.98038077 0.96087277 0.93978298 0.91713792\n",
      " 0.89298052 0.86737001 0.84038192 0.81210774 0.78265446 0.75214338\n",
      " 0.72070938 0.68849909 0.65566933 0.62238538 0.58881873 0.55514455\n",
      " 0.52153981 0.4881804  0.45523876 0.42288151 0.39126703 0.36054322\n",
      " 0.33084565 0.30229566 0.27499905 0.24904492 0.22450483 0.20143245\n",
      " 0.1798636  0.15981644 0.14129217 0.12427604 0.10873855 0.09463691\n",
      " 0.08191671 0.0705137  0.06035557 0.05136392 0.043456   0.03654652\n",
      " 0.03054927 0.02537862 0.02095082 0.01718517 0.0140049  0.01133791\n",
      " 0.00911731 0.00728173]\n",
      "For the chosen value of eta, which is 0.0, this results in the following sigma_t schedule for ddim sampler tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], dtype=torch.float64)\n",
      "Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\n",
      "Running DDIM Sampling with 45 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding image: 100%|██████████| 45/45 [00:05<00:00,  8.19it/s]\n"
     ]
    }
   ],
   "source": [
    "init_image = \"assets/stable-samples/depth2img/old_man.png\"\n",
    "input_image = Image.open(init_image)\n",
    "prompt = \"cyberpunk woman\"\n",
    "steps = 50\n",
    "num_samples = 1\n",
    "scale = 9.0\n",
    "strength = 0.9\n",
    "seed = 8888\n",
    "eta = 0.0\n",
    "\n",
    "img = predict(input_image, prompt, steps, num_samples, scale, seed, eta, strength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45ff0cb7-5deb-40f4-8bfb-cd09e330f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "img[0].save(\"depth.png\")\n",
    "img[1].save(\"output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36608c86-79ba-4de2-b471-8a3ba05fdd87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
